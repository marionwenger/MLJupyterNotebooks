{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd2fee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%autosave 5\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c57e1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69498008",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.genfromtxt(\"Icecream_temperatures.csv\", delimiter=\",\") # instead of pandas, we use numpy to load the data\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()  # creates a figure and an axes object\n",
    "ax.plot(points[:,0], points[:,1], 'ro') # ro means red points # xi are on axe 0 (= temp) and y are on axe 1 (= ice cream)\n",
    "ax.axis([30, 110, 0, 110]) # set the limits of the axis\n",
    "ax.set(xlabel='Temperature', ylabel='Ice cream sales',\n",
    "       title='Relationship between Temperature and Ice Cream Scores')\n",
    "ax.grid() # show the grid\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f362fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()  # creates a figure and an axes object\n",
    "ax.plot(points[:,1], points[:,0], 'ro') # data dimensions switched\n",
    "ax.axis([0, 110,30, 110, ]) # set the limits of the axis\n",
    "ax.set(ylabel='Temperature', xlabel='Ice cream sales',\n",
    "       title='Relationship between Temperature and Ice Cream Scores')\n",
    "ax.grid() # show the grid\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de89bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression   # sklearn is a library for ML models\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from importlib import reload   \n",
    "plt=reload(plt)                # a hack needed because previously we assigned an object (namely, \"ax\") to \"plt\"\n",
    "\n",
    "\n",
    "x=points[1:,0] #here we remove the heading (text) - see csv!\n",
    "y=points[1:,1] #here we remove the heading (text) - see csv!\n",
    "\n",
    "# Split the data into training/testing sets\n",
    "x_train = x[:-20]\n",
    "x_test = x[-20:]\n",
    "\n",
    "# Split the targets into training/testing sets # do you always split like that along the xs?\n",
    "y_train = y[:-20]\n",
    "y_test = y[-20:]\n",
    "\n",
    "# Create linear regression object\n",
    "model = LinearRegression(fit_intercept=True) # Whether to calculate the intercept for this model. \n",
    "                                             # If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).\n",
    "                                             # intercept = the constant!, false --> line through (0,0)\n",
    "# Train the model using the training sets\n",
    "model.fit(x_train[:, np.newaxis], y_train) # numpy.newaxis is used to increase the dimension of the existing array by one more dimension, when used once.\n",
    "\n",
    "x_new = x_train[:, np.newaxis]\n",
    "x_new[:] # it's two dimensional now, no data on the second dimension\n",
    "# i do not why we need it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1477aec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xfit = np.linspace(39, 100, 1000) # must be a guess based on former plot...\n",
    "                                  # it should be possible to read the coefficient out of the model???\n",
    "yfit = model.predict(xfit[:, np.newaxis]) # model was already trained on data, see above\n",
    "plt.scatter(x,y) # scatter plot just means a normal plot with points... based on the data x,y\n",
    "                 # x and y contain all the data, train and test data\n",
    "\n",
    "plt.plot(xfit, yfit,'r'); # r for red\n",
    "plt.ylabel(\"Ice cream sales\")\n",
    "plt.xlabel(\"Temperature\")\n",
    "plt.title(\"Relationship between Temperature and Ice Cream Sales\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ad3a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions for the test set \n",
    "print(x_test)\n",
    "reshaped_arr = x_test.reshape(-1, 1)  # needs to be reshaped as a column for the function to work\n",
    "#print(reshaped_arr)\n",
    "predicted_sales = model.predict(reshaped_arr )\n",
    "print(predicted_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a016ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_test, predicted_sales,'o'); # that is - real numbers versus predicted ones\n",
    "\n",
    "plt.ylabel(\"predicted sales\")\n",
    "plt.xlabel(\"real sales\")\n",
    "\n",
    "# impression after first glance:\n",
    "# eindeutig angeordnet um die Aches y = x, daher gut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c633fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x_test, predicted_sales-y_test,'o'); #residuals\n",
    "\n",
    "plt.ylabel(\"predicted sales-real sales\")\n",
    "plt.xlabel(\"temperature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97052b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(r): # sum of absolute residuals - TODO could of course use sum of squares etc.\n",
    "  l = 0\n",
    "  for ri in r:\n",
    "    l = l + abs(ri)\n",
    "  return l\n",
    "\n",
    "def loss_by_test_split(x, y, split):\n",
    "\n",
    "    # Split the data into training/testing sets\n",
    "    x_train = x[:-split]\n",
    "    x_test = x[-split:]\n",
    "\n",
    "    # Split the targets into training/testing sets \n",
    "    y_train = y[:-split]\n",
    "    y_test = y[-split:]\n",
    "\n",
    "    # Create linear regression object\n",
    "    model = LinearRegression(fit_intercept=True) # Whether to calculate the intercept for this model. \n",
    "                                             # If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).\n",
    "                                             # intercept = the constant!, false --> line through (0,0)\n",
    "    # Train the model using the training sets\n",
    "    model.fit(x_train[:, np.newaxis], y_train) # numpy.newaxis is used to increase the dimension of the existing array by one more dimension, when used once.\n",
    "   \n",
    "    xfit = np.linspace(39, 100, 1000) # must be a guess based on former plot...\n",
    "                                  # it should be possible to read the coefficient out of the model???\n",
    "    yfit = model.predict(xfit[:, np.newaxis]) # model was already trained on data, see above\n",
    "    \n",
    "    # Create predictions for the test set \n",
    "    reshaped_arr = x_test.reshape(-1, 1)  # needs to be reshaped as a column for the function to work\n",
    "    predicted_sales = model.predict(reshaped_arr )\n",
    "    \n",
    "    # residuals\n",
    "    r = predicted_sales-y_test\n",
    "    \n",
    "    return loss(r)\n",
    "\n",
    "def vary_loss_by_test_split(x, y):\n",
    "    z = np.empty([x.size, 3])\n",
    "    for i in range(1, x.size, 1):\n",
    "         z[i] = [i, x.size-i, loss_by_test_split(x, y, i)] # place of split / number of test data / loss \n",
    "    return z\n",
    "\n",
    "points = np.genfromtxt(\"Icecream_temperatures.csv\", delimiter=\",\") # instead of pandas, we use numpy to load the data\n",
    "x=points[1:,0] #here we remove the heading (text) - see csv!\n",
    "y=points[1:,1] #here we remove the heading (text) - see csv!\n",
    "z = vary_loss_by_test_split(x, y)\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d78f093",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt=reload(plt) \n",
    "plt.plot(z[:,1], z[:,2],'o')\n",
    "\n",
    "plt.xlabel(\"size of test data\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.title(\"Relationship between Size of Test Data and Loss\")\n",
    "plt.show()\n",
    "\n",
    "# i do not understand this, I would have expected the min loss to be somewhere in the middle\n",
    "# but maybe this is because of the linear nature... it is easy to guess the correct line...?\n",
    "# the improvement is not much after some initial data\n",
    "# that makes sense..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9671bf38",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,) into shape (3,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 95\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m     94\u001b[0m points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mgenfromtxt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIcecream_temperatures.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# instead of pandas, we use numpy to load the data\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m main(points)\n",
      "Cell \u001b[0;32mIn[2], line 87\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(points)\u001b[0m\n\u001b[1;32m     85\u001b[0m x\u001b[38;5;241m=\u001b[39mpoints[\u001b[38;5;241m1\u001b[39m:,\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#here we remove the heading (text) - see csv!\u001b[39;00m\n\u001b[1;32m     86\u001b[0m y\u001b[38;5;241m=\u001b[39mpoints[\u001b[38;5;241m1\u001b[39m:,\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m#here we remove the heading (text) - see csv!\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m plot_size_test_data_vs_loss(x, y)\n\u001b[1;32m     88\u001b[0m get_optimal_split_based_on_loss(x, y)\n",
      "Cell \u001b[0;32mIn[2], line 77\u001b[0m, in \u001b[0;36mplot_size_test_data_vs_loss\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_size_test_data_vs_loss\u001b[39m(x, y):\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     z \u001b[38;5;241m=\u001b[39m vary_loss_by_test_split(x, y)\n\u001b[1;32m     78\u001b[0m     plt\u001b[38;5;241m.\u001b[39mplot(z[:,\u001b[38;5;241m1\u001b[39m], z[:,\u001b[38;5;241m2\u001b[39m],\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     79\u001b[0m     plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msize of test data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 49\u001b[0m, in \u001b[0;36mvary_loss_by_test_split\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# debugging\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m#for n in range(x.size-1, 2, -1):\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m#  print(n) \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m#print(x[:-1])\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#stop\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(x\u001b[38;5;241m.\u001b[39msize\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m      z[i] \u001b[38;5;241m=\u001b[39m [i, loss_by_test_split(x, y, i)] \u001b[38;5;66;03m# size of test data / loss \u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m z\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,) into shape (3,)"
     ]
    }
   ],
   "source": [
    "def loss(r): # sum of absolute residuals - TODO could of course use sum of squares etc. - use library!!!\n",
    "  l = 0\n",
    "  for ri in r:\n",
    "    l = l + abs(ri)\n",
    "  return l\n",
    "\n",
    "def loss_by_test_split(x, y, split):\n",
    "\n",
    "    # Split the data into training/testing sets\n",
    "    x_train = x[:-split]\n",
    "    x_test = x[-split:]\n",
    "\n",
    "    # Split the targets into training/testing sets \n",
    "    y_train = y[:-split]\n",
    "    y_test = y[-split:]\n",
    "\n",
    "    # Create linear regression object\n",
    "    model = LinearRegression(fit_intercept=True) # Whether to calculate the intercept for this model. \n",
    "                                             # If set to False, no intercept will be used in calculations (i.e. data is expected to be centered).\n",
    "                                             # intercept = the constant!, false --> line through (0,0)\n",
    "    # Train the model using the training sets\n",
    "    model.fit(x_train[:, np.newaxis], y_train) # numpy.newaxis is used to increase the dimension of the existing array by one more dimension, when used once.\n",
    "   \n",
    "    xfit = np.linspace(39, 100, 1000) # must be a guess based on former plot...\n",
    "                                  # it should be possible to read the coefficient out of the model???\n",
    "    yfit = model.predict(xfit[:, np.newaxis]) # model was already trained on data, see above\n",
    "    \n",
    "    # Create predictions for the test set \n",
    "    reshaped_arr = x_test.reshape(-1, 1)  # needs to be reshaped as a column for the function to work\n",
    "    predicted_sales = model.predict(reshaped_arr )\n",
    "    \n",
    "    # residuals\n",
    "    r = predicted_sales-y_test\n",
    "    \n",
    "    return loss(r)\n",
    "\n",
    "def vary_loss_by_test_split(x, y):\n",
    "    z = np.empty([x.size, 3])\n",
    "    \n",
    "    # debugging\n",
    "    #for n in range(x.size-1, 2, -1):\n",
    "    #  print(n) \n",
    "    # x_train = x[:-split]\n",
    "    #print(x[:-364])\n",
    "    #print(x[:-1])\n",
    "    #stop\n",
    "    \n",
    "    for i in range(x.size-1, 2, -1):\n",
    "         z[i] = [i, loss_by_test_split(x, y, i)] # size of test data / loss \n",
    "    return z\n",
    "\n",
    "def get_optimal_split_based_on_loss(x,y):\n",
    "    z = np.empty([x.size, 2])\n",
    "    z = vary_loss_by_test_split(x, y)\n",
    "    min_loss = np.min(z, axis=1)\n",
    "    \n",
    "    # debugging\n",
    "    print('min_loss:', min_loss)\n",
    "    quit()\n",
    "\n",
    "    best_splits = set()\n",
    "    # solution by chat gpt\n",
    "    # Get the indices where the value in the second column of z is equal to min_loss\n",
    "    indices = np.where(z[:, 1] == min_loss)[0]\n",
    "    # Add these indices to the set best_splits\n",
    "    best_splits.update(indices)\n",
    "    \n",
    "    if len(best_splits) == 1:\n",
    "      print(\"The lowest loss\" + min_loss + \"is achieved with a split at place\" + best_splits + \"!\")\n",
    "    elif len(best_splits) == 0:\n",
    "      print(\"There is an error, no index with minimal loss found...\")\n",
    "    else:\n",
    "      print(\"The lowest loss\" + min_loss + \"is achieved with a split at the following places:\" + best_splits + \"!\")\n",
    "    \n",
    "def plot_size_test_data_vs_loss(x, y):\n",
    "    import matplotlib.pyplot as plt\n",
    "    z = vary_loss_by_test_split(x, y)\n",
    "    plt.plot(z[:,1], z[:,2],'o')\n",
    "    plt.xlabel(\"size of test data\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"Relationship between Size of Test Data and Loss\")\n",
    "    plt.show()\n",
    "\n",
    "def main(points):\n",
    "    x=points[1:,0] #here we remove the heading (text) - see csv!\n",
    "    y=points[1:,1] #here we remove the heading (text) - see csv!\n",
    "    plot_size_test_data_vs_loss(x, y)\n",
    "    get_optimal_split_based_on_loss(x, y)\n",
    "\n",
    "from sklearn.linear_model import LinearRegression   # sklearn is a library for ML models\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from importlib import reload   \n",
    "import numpy as np\n",
    "points = np.genfromtxt(\"Icecream_temperatures.csv\", delimiter=\",\") # instead of pandas, we use numpy to load the data\n",
    "main(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ea1ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
